{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vu93d2olS3m"
      },
      "source": [
        "# Design and Train an object detector to detect objects\n",
        "\n",
        "You have to design and implement a Training Pipeline that can train, test and visualize the model using the dataset provided.\n",
        "\n",
        "## Assignment Protocols\n",
        "\n",
        "- We expect it to take ~4 hours, with an extra 15 min for clear loom explanation(s)\n",
        "  - The assessment is timeboxed at 5 hours total in a single block. So please plan accordingly\n",
        "- You need to use Google Collaboratory to run and edit this notebook\n",
        "- You can only use Python as a programming Language\n",
        "- You cannot take help from any other person\n",
        "- You can use Google to search for references\n",
        "- You can not search on google for design-related things, like what should be loss function, or what should be model architecture.\n",
        "  - But you can use pre-trained backbones from PyTorch\n",
        "- Record a 5-10 mins of code walkthrough of the work you have done. You can use Loom Platform (https://www.loom.com) to record the video.\n",
        "  - Design Decisions\n",
        "    - Model Design which layers and activation functions you used and why\n",
        "    - Loss function, which loss functions you used and why\n",
        "    - Metrics, which metrics and why\n",
        "  - Any optimizations you have made to the codebase\n",
        "  - How you implemented resume functionality, what were the things you thought would be needed to resume training from exact same point\n",
        "  - Explain what parts of the assessment are completed and what is missing?\n",
        "  - Make sure to submit the screen recording link in the submission after you are done recording\n",
        "  - Please note that the free plan on Loom only allows for videos up to 5 minutes in length. As such, you may need to record two separate 5-minute videos.\n",
        "- [NO SUBMISSION WILL BE ACCEPTED WITHOUT]\n",
        "  - Trained best model weights\n",
        "  - Visualize Function in the Notebook\n",
        "  - Code Walk-through video\n",
        "\n",
        "## Task Details\n",
        "Design a Training Pipeline to train a object detector with following specs or assumptions:\n",
        "- Implement & Design Model\n",
        "  - You can use any backbone\n",
        "    - Either from PyTorch (torhvision) or any resource online\n",
        "    - But you need to design head your self (head means how you will use features of the back bone and get the desired outputs)\n",
        "  - Model needs to detect one object in each image\n",
        "  - Model should output following for each image passed as input:\n",
        "    - Whether we have an object or not\n",
        "    - Where is the object?\n",
        "      - The bounding box output format should be xmin, ymin, xmax, ymax\n",
        "      - It is not necessary the model is trained to output exactly this format but the visualize function which shows output should output in this format\n",
        "    - Either the object is a cat or dog?\n",
        "    - And which specie the object belongs to? There are in total 9 species: \n",
        "      - Cat [3 species]:\n",
        "        - Abyssinian\n",
        "        - Birman\n",
        "        - Persian\n",
        "      - Dog [6 species]\n",
        "        - american_bulldog\n",
        "        - american_pit_bull_terrier\n",
        "        - basset_hound\n",
        "        - beagle\n",
        "        - chihuahua\n",
        "        - pomeranian\n",
        "- Implement Custom Dataloader\n",
        "  - This is obvious as dataset is in a unique format any predifined dataloader wont work\n",
        "  - Follow best practices of writing custom dataloaders\n",
        "  - Details of the format of the dataset are defined in the Dataset Details section below\n",
        "  - Add needed pre-processing that you think would help train a better model or would help as we are using pre-trained weights as starting point\n",
        "  - Add augmentations that you think would help train a better model\n",
        "- Implement Loss Function\n",
        "  - Design and implement a loss function that can handle all of the outputs we have\n",
        "  - You can use pytorch built-in loss functions\n",
        "  - There are many scenarios which you need to handle, which one can understand from the dataset details and the model design\n",
        "- Implement Test Function\n",
        "  - The test function should be able to run the model on the validation set and output the metrics for all the outputs of the model\n",
        "  - Select the metrics carefully, there are many scenarios which can change the selection of a metric\n",
        "  - Keep in mind there are multiple outputs, you would need a metric for each output\n",
        "  - [NOTE] You don't need to implement metrics for the bounding box output as it can take more time than provided for this assessment. But please add details of the metrics you would have implemented in your code-walk through loom video.\n",
        "- Update Resume Training Functionality using the best weights\n",
        "  - Current script does not have save best weights functionality\n",
        "  - The code should be able to resume training from exactly same point from where the training was stopped if model weights file is passed\n",
        "  - Keep in mind you can not resume training from same point by just loading weights of the model\n",
        "- Implement a visualize function [Most important, without this no submission will be accepted]\n",
        "  - The input of the function should be path of a folder with images and the weight file\n",
        "    - Also the output folder path to save outputs\n",
        "  - This function should return a dictionary of dictionaries with following details for each image:\n",
        "    - {\n",
        "        \"has_object\": True,\n",
        "        \"cat_or_dog\": \"cat\",\n",
        "        \"specie\": \"persian\",\n",
        "        \"xmin\": 10,\n",
        "        \"ymin\": 10,\n",
        "        \"xmax\": 10,\n",
        "        \"ymax\": 10\n",
        "    }\n",
        "  - And in case there is no object it should have 0 for bbox values, \"NA\" for \"cat_or_dog\" and \"specie\", and False for \"has_object\".\n",
        "  - Values of the returned dictionary should be like explained above and keys should be image names including the extension \".jpg\" or \".jpeg\"\n",
        "  - Should save output image with bounding box drawn on it, with same name input image but place in the output folder \n",
        "- Try to train the best model\n",
        "\n",
        "\n",
        "## Dataset Details\n",
        "The dataset has in total 1041 images. Each image has a single object which is either a cat or a dog.\n",
        "- There are multiple species for both cat and dog.\n",
        "- The number of images falling in each specie is as follows:\n",
        "  - basset_hound: 93\n",
        "  - Birman: 93\n",
        "  - pomeranian: 93\n",
        "  - american_pit_bull_terrier: 93\n",
        "  - american_bulldog: 93\n",
        "  - Abyssinian: 92\n",
        "  - beagle: 93\n",
        "  - Persian: 93\n",
        "  - chihuahua: 93\n",
        "  - empty: 142\n",
        "- The dataset has two folders:\n",
        "  - images\n",
        "    - Inside images folder we have 986 images in .jpg folder\n",
        "  - labels\n",
        "    - Inside labels folder we have 899 .xml files each file with details of image labels\n",
        "    - For any image that does not have a cat or dog, there is no corresponding xml file\n",
        "\n",
        "## Deliverable\n",
        "- Updated Colab Based Jupyter Notebook:\n",
        "  - With all the required functionality Implemented\n",
        "  - Which one can train the model without any errors\n",
        "  - One should achieve same metrics (Almost same metrics) if I run training using this collab notebook\n",
        "    - Set default values for everything accordingly in the notebook\n",
        "  - During evaluation we will just run the notebook and use the best weights the notebook saves automatically\n",
        "- Best weights you have trained\n",
        "  - We will Evaluate your weights against hold-out test we have and compare results\n",
        "  - We will use visualize function to generate outputs for each image\n",
        "  - Upload weights in an easily downloadable location like, Dropbox, Google Drive, Github, etc\n",
        "- A video code-walk through explaining your design decisions including but not limited to:\n",
        "  - Model Design which layers and activation functions you used and why\n",
        "  - Loss function, which loss functions you used and why\n",
        "  - Metrics, which metrics and why\n",
        "  - Any optimizations you have made to the codebase\n",
        "  - How you implemented resume functionality, what were the things you thought would be needed to resume training from exact same point\n",
        "\n",
        "\n",
        "## Evaluation Criteria\n",
        " - Design Decisions\n",
        " - Completeness: Did you include all features?\n",
        " - Correctness: Does the solution (all deliverables) work in sensible, thought-out ways?\n",
        " - Maintainability: Is the code written in a clean, maintainable way?\n",
        " - Testing: Is the solution adequately tested?\n",
        " - Documentation: Is the codebase well-documented and has proper steps to run any of the deliverables?\n",
        "\n",
        "## Extra Points\n",
        "- Add metrics for the Bounding Box Output\n",
        "- Any Updates in the notebook (Bugs/Implementation Mistakes etc)\n",
        "\n",
        "## How to submit\n",
        "- Please upload the Notebook for this project to GitHub, and post a link to your repository below [repo link box, on the left of submit button].\n",
        "  - Create a new GitHub repository from scratch\n",
        "  - Add the final Colab/Jupyter notebook to the repository\n",
        "- Please upload video and your final best weights on Google Drive or any other platform, and paste the link to the folder with both video and model in the text box just above the submit button.\n",
        "- Please paste the commit Id of the latest commit of your Github Repo, which should not be later than 5 hours of time when the repo was created.\n",
        "  - Please note the submission without the commit id will not be considered."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE_rGh4kod4n"
      },
      "source": [
        "# Install Required Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "magQ0ErkoOIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05d800a1-124c-42ed-eba6-da29c1e04fc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.2)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.13)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.11.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.26.15)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.4.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n",
            "Building wheels for collected packages: bs4\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1257 sha256=c3fb9aa213b88674107dbb0a93e52c719319a0d9e977fff653ef602a04e4f901\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
            "Successfully built bs4\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.1\n"
          ]
        }
      ],
      "source": [
        "! pip install bs4 lxml kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw3F4t1eaLl6"
      },
      "source": [
        "# Download Dataset from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "59RSpyzRaOVj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = 'bilalyousaf0014'\n",
        "os.environ['KAGGLE_KEY'] = '11031bc21c5e3ec23585dbe17dc4267d'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "plZEhi_oaPPn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01334302-efc6-4265-c0c0-97e6af2a05a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ml-engineer-assessment-dataset.zip to /content\n",
            " 89% 70.0M/78.6M [00:00<00:00, 65.2MB/s]\n",
            "100% 78.6M/78.6M [00:00<00:00, 83.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d bilalyousaf0014/ml-engineer-assessment-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6jkCK0uaxRT"
      },
      "outputs": [],
      "source": [
        "! unzip /content/ml-engineer-assessment-dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMEbUqsPl9s4"
      },
      "source": [
        "# MODEL IMPLEMENTATION:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uXb6eKgFmNzx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "from torchvision.models import resnet18, ResNet18_Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BtSgG4OklRte"
      },
      "outputs": [],
      "source": [
        "# class Model(nn.Module):\n",
        "\n",
        "#   def __init__(self):\n",
        "#     super(Model, self).__init__()\n",
        "#     pretrained_model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "#     self.backbone = nn.Sequential(*list(pretrained_model.children())[:-2])\n",
        "#     ### Initialize the required Layers\n",
        "#     self.have_object = None\n",
        "#     self.cat_or_dog = None\n",
        "#     self.specie = None\n",
        "#     self.bbox = None\n",
        "#     ### Initialize the required Layers\n",
        "\n",
        "#   def forward(self, input):\n",
        "#       out_backbone = self.backbone(input)\n",
        "#       ### Write Forward Calls for the Model\n",
        "\n",
        "#       return {\n",
        "#           \"bbox\": None,\n",
        "#           \"object\": None,\n",
        "#           \"cat_or_dog\": None,\n",
        "#           \"specie\": None\n",
        "#       }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    pretrained_model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "    self.backbone = nn.Sequential(*list(pretrained_model.children())[:-2])\n",
        "    ### Initialize the required Layers\n",
        "    self.have_object= nn.Linear(512,1)\n",
        "    self.cat_or_dog= nn.Linear(512,2)\n",
        "    self.specie= nn.Linear(512,9)\n",
        "    self.bbox= nn.Linear(512,4)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, input):\n",
        "      out_backbone = self.backbone(input)\n",
        "      out_backbone = out_backbone.view(out_backbone.size(0), -1)\n",
        "\n",
        "      ### Write Forward Calls for the Model\n",
        "      object_presence = torch.sigmoid(self.have_object(out_backbone))\n",
        "      cat_or_dog_logits = self.cat_or_dog(out_backbone)\n",
        "      cat_or_dog_probablities = torch.softmax(cat_or_dog_logits , dim=1)\n",
        "      specie_logits = self.specie(out_backbone)\n",
        "      specie_probablities = torch.softmax(specie_logits , dim=1)\n",
        "      bounding_box = self.bbox(out_backbone)\n",
        "\n",
        "      return {\n",
        "          \"bbox\": bounding_box,\n",
        "          \"object\": object_presence,\n",
        "          \"cat_or_dog\": cat_or_dog_probablities,\n",
        "          \"specie\": specie_probablities\n",
        "      }\n"
      ],
      "metadata": {
        "id": "w6wlDhwmTXrc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avxCQ2W-oGxV"
      },
      "source": [
        "# CUSTOM DATALOADER IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "ZNds6zOg8w5A"
      },
      "outputs": [],
      "source": [
        "train_list = np.load('/content/assessment_dataset/train_list.npy', allow_pickle=True).tolist()\n",
        "val_list = np.load('/content/assessment_dataset/val_list.npy', allow_pickle=True).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "E30zu6ICmKOL"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def read_xml_file(path):\n",
        "    with open(path, 'r') as f:\n",
        "        data = f.read()\n",
        "    bs_data = BeautifulSoup(data, 'xml')\n",
        "    return {\n",
        "        \"cat_or_dog\": bs_data.find(\"name\").text,\n",
        "        \"xmin\": int(bs_data.find(\"xmin\").text),\n",
        "        \"ymin\": int(bs_data.find(\"ymin\").text),\n",
        "        \"xmax\": int(bs_data.find(\"xmax\").text),\n",
        "        \"ymax\": int(bs_data.find(\"ymax\").text),\n",
        "        \"specie\": \"_\".join(path.split(os.sep)[-1].split(\"_\")[:-1])\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import cv2 as cv\n",
        "\n",
        "image_folder_path = os.path.join('/content/assessment_dataset', \"images\")\n",
        "img = os.listdir(image_folder_path)\n",
        "count = 0\n",
        "for i in img:\n",
        "  # print(i)\n",
        "  if i.split('.')[-1]!='jpg':\n",
        "    os.remove(os.path.join(image_folder_path,i))\n"
      ],
      "metadata": {
        "id": "DXTSvR96bCvX"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder_path = os.path.join('/content/assessment_dataset', \"images\")\n",
        "img = os.listdir(image_folder_path)\n",
        "len(img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yV8TRrBrGY5K",
        "outputId": "0c9ea9c9-4acd-4b12-e12d-dc62c900bcdd"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "845"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "z16n7BbnoviB"
      },
      "outputs": [],
      "source": [
        "# from PIL import Image\n",
        "\n",
        "# import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# class CustomDataset():\n",
        "\n",
        "#   def __init__(self, dataset_path, images_list, train=False):\n",
        "\n",
        "#     self.preprocess = None\n",
        "\n",
        "#     image_folder_path = os.path.join(dataset_path, \"images\")\n",
        "#     label_folder_path = os.path.join(dataset_path, \"labels\")\n",
        "\n",
        "#     for path in os.listdir(image_folder_path):\n",
        "#         name = path.split(os.sep)[-1].split(\".\")[0]\n",
        "#         if name in images_list:\n",
        "#           xml_path = os.path.join(label_folder_path, name+\".xml\")\n",
        "#           xml_data = read_xml_file(xml_path)\n",
        "\n",
        "#   def __len__(self):\n",
        "#     return None\n",
        "\n",
        "#   def __getitem__(self, index):\n",
        "#     image = self.preprocess(image)\n",
        "#     labels = None\n",
        "#     return image, label\n",
        "from PIL import Image\n",
        "xm = Image.open('/content/assessment_dataset/images/00001.jpeg')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#image preprocessing \n",
        "from torchvision import transforms\n",
        "preprocess_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485,0.456,0.406), (0.229,0.224,0.225))\n",
        "])\n",
        "augmentations = transforms.Compose([transforms.RandomHorizontalFlip()])"
      ],
      "metadata": {
        "id": "dIVncVL-cj0K"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "class CustomDataset():\n",
        "\n",
        "  def __init__(self, dataset_path, images_list, train=False, preprocess=None, augmentations = None):\n",
        "    self.dataset_path = dataset_path\n",
        "    self.images_list = images_list\n",
        "    self.train = train\n",
        "    self.preprocess = preprocess\n",
        "    self.augmentations = augmentations\n",
        "    self.image_folder_path = os.path.join(self.dataset_path, \"images\")\n",
        "    self.label_folder_path = os.path.join(self.dataset_path, \"labels\")\n",
        "    self.data = self._load_data()\n",
        "\n",
        "  def _load_data(self):\n",
        "    image_folder_path = os.path.join(self.dataset_path, \"images\")\n",
        "    label_folder_path = os.path.join(self.dataset_path, \"labels\")\n",
        "    data = []\n",
        "\n",
        "    for path in os.listdir(image_folder_path):\n",
        "        name = path.split(os.sep)[-1].split(\".\")[0]\n",
        "        if name in self.images_list:\n",
        "          data.append(name)\n",
        "          \n",
        "\n",
        "    return data\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    name = self.data[index]\n",
        "    xml_path = os.path.join(self.label_folder_path, name+\".xml\")\n",
        "    label_data = read_xml_file(xml_path)\n",
        "\n",
        "    #preprocess\n",
        "    # image_path_name = os.listdir(self.image_folder_path)\n",
        "    # img_name = image_path_name[index]\n",
        "    # print(img_name)\n",
        "    image_path = os.path.join(self.image_folder_path, name+\".jpg\")\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    if self.preprocess is not None:\n",
        "      image = self.preprocess(image)\n",
        "    \n",
        "    #augmentations \n",
        "    if self.train and self.augmentations is not None:\n",
        "      image = self.augmentations(image)\n",
        "\n",
        "    #label information extracted\n",
        "    object_present = True\n",
        "    cat_or_dog = label_data[\"cat_or_dog\"]\n",
        "    specie = label_data[\"specie\"]\n",
        "    xmin = label_data[\"xmin\"]\n",
        "    ymin = label_data[\"ymin\"]\n",
        "    xmax = label_data[\"xmax\"]\n",
        "    ymax = label_data[\"ymax\"]\n",
        "\n",
        "    #labels_dict\n",
        "    label = {\n",
        "        \"object_present\":object_present,\n",
        "        \"cat_or_dog\": cat_or_dog,\n",
        "        \"specie\":specie,\n",
        "        \"bbox\": [xmin, ymin, xmax, ymax],\n",
        "        }\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "fGlAoYJta_c9"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkbn3C3cqahY"
      },
      "source": [
        "# TRAINING LOOP IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxODf-DLqmPA"
      },
      "source": [
        "## Initializations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Drkrrx8pqjzM"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "1k8jvr7EqaPz"
      },
      "outputs": [],
      "source": [
        "# # import our library\n",
        "import torchmetrics\n",
        "\n",
        "# def train(epochs, model_weights):\n",
        "\n",
        "#   # Initialize Model and Optimizer\n",
        "#   model = Model()\n",
        "#   optimizer = None\n",
        "\n",
        "#   # Initialize Loss Functions\n",
        "#   have_object_loss = None\n",
        "#   specie_loss = None\n",
        "#   cat_or_dog_loss = None\n",
        "#   bbox_loss = None # Not necessary you need to apply function to all coordinates together, You can have separete loss functions for all coordinates too\n",
        "#   # Below or Above\n",
        "#   xmin_loss = None\n",
        "#   ymin_loss = None\n",
        "#   xmax_loss = None\n",
        "#   ymax_loss = None\n",
        "\n",
        "#   training_dataset = CustomDataset(\"/content/assessment_dataset\", images_list=train_list)\n",
        "#   training_loader = None\n",
        "\n",
        "#   if torch.cuda.is_available():\n",
        "#     model = model.cuda()\n",
        "\n",
        "#   def train_one_epoch(epoch_index, tb_writer):\n",
        "#       running_loss = 0.\n",
        "#       last_loss = 0.\n",
        "\n",
        "#       # Here, we use enumerate(training_loader) instead of\n",
        "#       # iter(training_loader) so that we can track the batch\n",
        "#       # index and do some intra-epoch reporting\n",
        "#       for i, data in enumerate(training_loader):\n",
        "#           # Every data instance is an input + label pair\n",
        "#           inputs, labels = data\n",
        "\n",
        "#           # Make predictions for this batch\n",
        "#           outputs = model(inputs)\n",
        "\n",
        "#           # Compute the loss and its gradients\n",
        "#           loss_have_object = have_object_loss(outputs[\"object\"], None)\n",
        "#           loss_specie = specie_loss(outputs[\"specie\"], None)\n",
        "#           loss_cat_or_dog = cat_or_dog_loss(outputs[\"cat_or_dog\"], None)\n",
        "          \n",
        "#           loss_bbox = bbox_loss(outputs[\"bbox\"], None)\n",
        "#           # Above or Below\n",
        "#           loss_xmin = xmin_loss(outputs[\"bbox\"], None)\n",
        "#           loss_ymin = ymin_loss(outputs[\"bbox\"], None)\n",
        "#           loss_xmax = xmax_loss(outputs[\"bbox\"], None)\n",
        "#           loss_ymax = ymax_loss(outputs[\"bbox\"], None)\n",
        "\n",
        "#           loss =  # Consolidate all individual losses\n",
        "\n",
        "#           # Gather data and report\n",
        "#           running_loss += loss.item()\n",
        "#           if i % 10 == 0:\n",
        "#               last_loss = running_loss / 10 # loss per batch\n",
        "#               running_loss = 0.\n",
        "#       return last_loss\n",
        "\n",
        "#   for i in range(epochs):\n",
        "\n",
        "#     epoch_loss = train_one_epoch(i, None)\n",
        "#     print(f' Epoch {i} Loss : {epoch_loss}')\n",
        "\n",
        "#     torch.save(\"model.pth\", model.state_dict())\n",
        "#     metrics = test(model)\n",
        "#     print(metrics)\n",
        "\n",
        "# def test(model, val_list):\n",
        "\n",
        "#   def post_process_object(x):\n",
        "#     return x\n",
        "\n",
        "#   def post_process_cat_or_dog(x):\n",
        "#     return x\n",
        "\n",
        "#   def post_process_specie(x):\n",
        "#     return x\n",
        "\n",
        "#   def post_process_bbox(x):\n",
        "#     return x\n",
        "\n",
        "#   val_dataset = CustomDataset(\"/content/assessment_dataset\", images_list=val_list)\n",
        "#   val_loader = None\n",
        "\n",
        "#   metric_object = None\n",
        "#   metric_cat_or_dog = None\n",
        "#   metric_specie = None\n",
        "#   metric_bbox = None\n",
        "\n",
        "#   for i, data in enumerate(val_loader):\n",
        "#     inputs, labels = data\n",
        "\n",
        "#     # Make predictions for this batch\n",
        "#     outputs = model(inputs)\n",
        "\n",
        "#   score_object = metric_object()\n",
        "#   score_cat_or_dog = metric_cat_or_dog()\n",
        "#   score_specie = metric_specie()\n",
        "#   score_bbox = metric_bbox()\n",
        "\n",
        "#   return score_object, score_cat_or_dog, score_specie, score_bbox"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchmetrics\n"
      ],
      "metadata": {
        "id": "u2BPth5_-7Wx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import our library\n",
        "import torchmetrics\n",
        "\n",
        "def train(epochs, model_weights):\n",
        "\n",
        "  # Initialize Model and Optimizer\n",
        "  model = Model()\n",
        "  if torch.cuda.is_available():\n",
        "    model=model.to(torch.device(\"cuda\"))\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "  # Initialize Loss Functions\n",
        "  have_object_loss = nn.BCELoss()\n",
        "  specie_loss = nn.CrossEntropyLoss()\n",
        "  cat_or_dog_loss = nn.CrossEntropyLoss()\n",
        "  bbox_loss = nn.MSELoss # Not necessary you need to apply function to all coordinates together, You can have separete loss functions for all coordinates too\n",
        "  # have_object_loss = None\n",
        "  # specie_loss = None\n",
        "  # cat_or_dog_loss = None\n",
        "  # bbox_loss = None # Not necessary you need to apply function to all coordinates together, You can have separete loss functions for all coordinates too\n",
        "\n",
        "  # # Below or Above\n",
        "  # xmin_loss = None\n",
        "  # ymin_loss = None\n",
        "  # xmax_loss = None\n",
        "  # ymax_loss = None\n",
        "  \n",
        "  training_dataset = CustomDataset(\"/content/assessment_dataset\", images_list=train_list,train=True, preprocess = preprocess_transform)\n",
        "  print('Hasssssss')\n",
        "  training_loader = DataLoader(training_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    # model = model.cuda()\n",
        "\n",
        "  def train_one_epoch(epoch_index, tb_writer):\n",
        "      running_loss = 0.0\n",
        "      last_loss = 0.0\n",
        "\n",
        "      # Here, we use enumerate(training_loader) instead of\n",
        "      # iter(training_loader) so that we can track the batch\n",
        "      # index and do some intra-epoch reporting\n",
        "      for i, data in enumerate(training_loader):\n",
        "          # Every data instance is an input + label pair\n",
        "          inputs, labels = data\n",
        "          # inputs = [tensor.to(device) for tensor in inputs]\n",
        "          # labels = [tensor.to(device) for tensor in labels]\n",
        "        #   # label = {\"object\":labels['object_present'].to(device),\n",
        "        #   #          \"cat_or_dog\": labels['cat_or_dog'].to(device),\n",
        "        #   #          \"specie\":labels['specie'].to(device),\n",
        "        #   #          \"bbox\": labels['bbox'].to(device),\n",
        "        # }\n",
        "\n",
        "          # if torch.cuda.is_available():\n",
        "          #   inputs = inputs.cuda()\n",
        "          #   labels = labels.cuda()\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Make predictions for this batch\n",
        "          outputs = model(inputs)\n",
        "\n",
        "          # Compute the loss and its gradients\n",
        "          loss_have_object = have_object_loss(outputs[\"object\"], labels['object_present'])\n",
        "          loss_specie = specie_loss(outputs[\"specie\"], labels['specie'])\n",
        "          loss_cat_or_dog = cat_or_dog_loss(outputs[\"cat_or_dog\"], labels['cat_or_dog'])\n",
        "          \n",
        "          loss_bbox = bbox_loss(outputs[\"bbox\"], labels['bbox'])\n",
        "          # Above or Below\n",
        "          # loss_xmin = xmin_loss(outputs[\"bbox\"], None)\n",
        "          # loss_ymin = ymin_loss(outputs[\"bbox\"], None)\n",
        "          # loss_xmax = xmax_loss(outputs[\"bbox\"], None)\n",
        "          # loss_ymax = ymax_loss(outputs[\"bbox\"], None)\n",
        "\n",
        "          # Consolidate all individual losses\n",
        "          loss =  (\n",
        "              loss_have_object\n",
        "              + loss_specie\n",
        "              + loss_cat_or_dog\n",
        "              + loss_bbox\n",
        "          )\n",
        "          #backward pass\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # Gather data and report\n",
        "          running_loss += loss.item()\n",
        "          if i % 10 == 0:\n",
        "              last_loss = running_loss / 10 # loss per batch\n",
        "              running_loss = 0.\n",
        "      return last_loss\n",
        "\n",
        "  for i in range(epochs):\n",
        "\n",
        "    epoch_loss = train_one_epoch(i, None)\n",
        "    print(f' Epoch {i} Loss : {epoch_loss}')\n",
        "\n",
        "    torch.save(\"model.pth\", model.state_dict())\n",
        "    metrics = test(model)\n",
        "    print(metrics)\n",
        "\n",
        "def test(model, val_list):\n",
        "\n",
        "  def post_process_object(x):\n",
        "    return x\n",
        "\n",
        "  def post_process_cat_or_dog(x):\n",
        "    return x\n",
        "\n",
        "  def post_process_specie(x):\n",
        "    return x\n",
        "\n",
        "  def post_process_bbox(x):\n",
        "    return x\n",
        "\n",
        "  val_dataset = CustomDataset(\"/content/assessment_dataset\", images_list=val_list)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "  metric_object = torchmetrics.Accuracy()\n",
        "  metric_cat_or_dog = torchmetrics.Accuracy()\n",
        "  metric_specie = torchmetrics.Accuracy()\n",
        "  metric_bbox = torchmetrics.MeanSquaredError()\n",
        "\n",
        "  for i, data in enumerate(val_loader):\n",
        "    inputs, labels = data\n",
        "\n",
        "    # if torch.cuda.is_available():\n",
        "    #   inputs = inputs.cuda()\n",
        "    #   labels = labels.cuda()\n",
        "\n",
        "    # Make predictions for this batch\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    #post-process\n",
        "    pred_object = post_process_object(outputs[\"object\"])\n",
        "    pred_cat_or_dog = post_process_cat_or_dog(outputs[\"cat_or_dog\"])\n",
        "    pred_specie = post_process_specie(outputs[\"specie\"])\n",
        "    pred_bbox = post_process_bbox(outputs[\"bbox\"])\n",
        "\n",
        "    metric_object(pred_object, labels[\"object_present\"])\n",
        "    metric_cat_or_dog(pred_cat_or_dog, labels[\"cat_or_dog\"])\n",
        "    metric_specie(pred_specie, labels[\"specie\"])\n",
        "    metric_bbox(pred_bbox, labels[\"bbox\"])\n",
        "\n",
        "  score_object = metric_object.compute()\n",
        "  score_cat_or_dog = metric_cat_or_dog.compute()\n",
        "  score_specie = metric_specie.compute()\n",
        "  score_bbox = metric_bbox.compute()\n",
        "\n",
        "  return score_object, score_cat_or_dog, score_specie, score_bbox"
      ],
      "metadata": {
        "id": "bJ_uChd11FoN"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "model_weights = \"/content/model/model.pth\"\n",
        "\n",
        "train(epochs,model_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "YWhmDuoq_EH_",
        "outputId": "3739d7fe-96e9-44df-f52d-b92de7a0d1ce"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hasssssss\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-118-dc4aeb3c087b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/model/model.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-117-f3f45807ff34>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, model_weights)\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf' Epoch {i} Loss : {epoch_loss}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-117-f3f45807ff34>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m           \u001b[0;31m# Make predictions for this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m           \u001b[0;31m# Compute the loss and its gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-25d496c8bf1e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0mout_backbone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m       \u001b[0mout_backbone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_backbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_backbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8TgRVGPDaUAd"
      },
      "outputs": [],
      "source": [
        "from PIL import ImageDraw\n",
        "\n",
        "def visualize(model_weights, image_folder_path, output_folder=\"output\"):\n",
        "\n",
        "  model = Model()\n",
        "  model.load_state_dict(torch.load(model_weights))\n",
        "\n",
        "  try:\n",
        "    image = Image.open(os.path.join(\"/content/assessment_dataset/images\", image_name+\".jpg\"))\n",
        "  except:\n",
        "    image = Image.open(os.path.join(\"/content/assessment_dataset/images\", image_name+\".jpeg\"))\n",
        "\n",
        "  preprocess = None\n",
        "  output = model()\n",
        "  return {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7npVwns6LU6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}